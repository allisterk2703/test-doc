{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documentation","text":"<p>https://github.com/allisterk2703/mlops-project</p>"},{"location":"#team-members","title":"Team Members","text":"<p>Allister KOHN allister.kohn@student-cs.fr</p> <p>Elizaveta VASILEVA elizaveta.vasileva@student-cs.fr</p> <p>Enzo PALOS enzo.palos@student-cs.fr</p> <p>Jinsuh YOU jinsuh.you@student-cs.fr</p>"},{"location":"#introduction","title":"Introduction","text":"<p>The project proposed to us as part of our MLOps course aims to design an MLOps workflow to industrialize the lifecycle of Machine Learning models. A classic lifecycle includes data ingestion, model training, evaluation, and deployment. The goal is to identify and implement key features to facilitate the management, scalability, and reproducibility of models in production. The project is divided into two parts:</p> <ol> <li>Write a specification detailing the chosen workflows and features</li> <li>Implement the corresponding code, to be hosted on GitHub.</li> </ol> <p>The objective is to structure a modular, clear, and reusable system, adopting a pragmatic and robust approach.</p> <p>To this end, the GitHub repository dsba-platform is a relevant starting point, as it offers a modular architecture that integrates the essential steps of an MLOps workflow. It also includes a CLI, an API (with FastAPI), and a Dockerfile, facilitating containerization, integration, and deployment to the cloud. Furthermore, its structure easily allows for adding advanced features that ensure a smooth and reproducible workflow.</p> <p>The code in this repository is suited to binary classification tasks, like the famous Titanic dataset, which predicts passenger survival based on their characteristics. Consequently, we have chosen to focus on a tool intended exclusively for binary classification tasks in order to remain focused on a clear and well-controlled scope.</p>"},{"location":"#tutorial-forker-et-configurer-le-projet-pour-la-premiere-fois","title":"Tutorial: forker et configurer le projet pour la premi\u00e8re fois","text":"<p>https://github.com/JoachimZ/dsba-platform</p> <ul> <li>Cliquer sur <code>Fork</code> depuis la page GitHub, puis sur <code>Create fork</code></li> <li>R\u00e9cup\u00e9rer le lien du d\u00e9p\u00f4t fork\u00e9 dans son propre espace GitHub</li> <li>Dans un terminal, ex\u00e9cuter la commande : <code>cd Desktop &amp;&amp; git clone &lt;repo_url&gt;</code></li> <li> <p>Toujours dans le terminal, ex\u00e9cuter les commandes (remplacer par le chemin o\u00f9 est situ\u00e9 le dossier <code>dsba-platform</code>) :</p> <p><code>echo 'export PYTHONPATH=\"$PYTHONPATH:/Users/allisterkohn/Desktop/dsba-platform/src\"' &gt;&gt; ~/.zshrc</code></p> <p><code>echo \"export DSBA_MODELS_ROOT_PATH='/Users/allisterkohn/Desktop/dsba-platform/models'\" &gt;&gt; ~/.zshrc</code></p> </li> <li> <p>Ouvrir VS Code, aller dans <code>*Fichier &gt; Ouvrir le dossier</code>* et ouvrir le dossier <code>dsba-platform</code></p> </li> <li>Taper <code>Control ^</code> + <code>Shift \u21e7</code> + <code>&lt;</code> pour ouvrir un terminal dans VS Code</li> <li>Cr\u00e9er un environnement virtuel .venv et installer les packages n\u00e9cessaires en ex\u00e9cutant successivement les commandes suivantes :<ul> <li><code>python -m venv .venv</code></li> <li><code>echo \".venv/\" &gt;&gt; .gitignore</code></li> <li><code>source .venv/bin/activate</code></li> <li><code>pip install hatch</code></li> <li><code>hatch env create</code></li> <li><code>pip install -e .</code></li> </ul> </li> <li>V\u00e9rifier que tout fonctionne en ex\u00e9cutant dans le terminal de VS Code la commande : <code>python src/cli/dsba_cli list</code></li> </ul>"},{"location":"#setup-du-depot-forke","title":"Setup du d\u00e9p\u00f4t fork\u00e9","text":"<ul> <li>Ouvrir le projet dans VS Code, puis taper <code>Control ^</code> + <code>Shift \u21e7</code> + <code>&lt;</code> pour ouvrir un terminal dans VS Code</li> <li>Cr\u00e9er un environnement virtuel .venv et installer les packages n\u00e9cessaires en ex\u00e9cutant successivement dans le terminal lanc\u00e9 pr\u00e9c\u00e9demment les commandes suivantes :<ul> <li><code>python -m venv .venv</code></li> <li><code>source .venv/bin/activate</code></li> <li><code>pip install hatch</code></li> <li><code>hatch env create</code></li> <li><code>pip install -e .</code></li> </ul> </li> <li> <p>Cr\u00e9er un fichier <code>.env</code> \u00e0 la racine du projet et y ins\u00e9rer les \u00e9l\u00e9ments suivants :</p> <p>https://signin.aws.amazon.com/signin?redirect_uri=https%3A%2F%2Feu-west-3.console.aws.amazon.com%2Fconsole%2Fhome%3FhashArgs%3D%2523%26isauthcode%3Dtrue%26region%3Deu-west-3%26state%3DhashArgsFromTB_eu-west-3_f832d75c15eb7d4a&amp;client_id=arn%3Aaws%3Asignin%3A%3A%3Aconsole%2Fcanvas&amp;forceMobileApp=0&amp;code_challenge=bE_YlZZ6wzlaVIe1sWQhQrBRCIvTRRUlpOTwVqOJmFI&amp;code_challenge_method=SHA-256</p> <pre><code>AWS_ACCESS_KEY=&lt;YOUR_AWS_ACCESS_KEY&gt;\nAWS_SECRET_KEY=&lt;YOUR_AWS_SECRET_KEY&gt;\nAWS_REGION=&lt;YOUR_AWS_REGION&gt;\nS3_BUCKET_NAME=&lt;YOUR_S3_BUCKET_NAME&gt;\n</code></pre> </li> </ul>"},{"location":"#features","title":"Features","text":""},{"location":"#ajouter-un-dataset","title":"Ajouter un dataset","text":"<p>Permet d\u2019ajouter un dataset \u00e0 l\u2019outil.</p> <ul> <li> <p>Param\u00e8tres :</p> <ul> <li><code>--name</code> (obligatoire) : Nom attribu\u00e9 au dataset lors de son ajout.</li> <li><code>--file</code> : Chemin vers un fichier local contenant le dataset.</li> <li><code>--url</code> : URL d\u2019une source distante pour r\u00e9cup\u00e9rer le dataset.</li> <li><code>--new</code> : Indique qu'il s'agit d'un nouveau dataset (\u00e0 ne pas inclure pour une nouvelle version d\u2019un dataset existant).</li> </ul> <p>Remarques :</p> <ul> <li>L'un des param\u00e8tres <code>--file</code> ou <code>--url</code> doit \u00eatre sp\u00e9cifi\u00e9, mais pas les deux simultan\u00e9ment.</li> <li>Le flag <code>--new</code> est n\u00e9cessaire pour ajouter un dataset qui n\u2019existe pas encore.</li> <li>Exemples :</li> <li> <p>Ajouter un dataset depuis un fichier local :</p> <pre><code>python src/cli/dsba_cli save_dataset --name titanic --file /Users/allisterkohn/Desktop/titanic.csv\n</code></pre> </li> <li> <p>Ajouter un dataset depuis une URL :</p> <pre><code>python src/cli/dsba_cli save_dataset --name titanic --url https://www.kaggle.com/api/v1/datasets/download/yasserh/titanic-dataset\n</code></pre> </li> </ul> </li> </ul>"},{"location":"#afficher-tous-les-datasets-locaux","title":"Afficher tous les datasets locaux","text":"<p>Affiche la liste de tous les datasets enregistr\u00e9s localement sur la machine.</p> <ul> <li> <p>Exemple :</p> <pre><code>python src/cli/dsba_cli list_local_datasets\n</code></pre> </li> </ul>"},{"location":"#afficher-les-versions-dun-dataset-local-specifique","title":"Afficher les versions d\u2019un dataset local sp\u00e9cifique","text":"<p>Permet de lister toutes les versions d\u2019un dataset sp\u00e9cifique stock\u00e9 localement.</p> <ul> <li>Param\u00e8tre :<ul> <li><code>--dataset</code> (obligatoire) : Nom du dataset.</li> </ul> </li> <li> <p>Exemple :</p> <pre><code>python src/cli/dsba_cli list_dataset_versions --dataset titanic\n</code></pre> </li> </ul>"},{"location":"#afficher-tous-les-datasets-s3","title":"Afficher tous les datasets S3","text":"<p>Affiche la liste des datasets stock\u00e9s dans le bucket S3 <code>dsba-mlops-project-bucket</code>, disponibles pour t\u00e9l\u00e9chargement.</p> <ul> <li> <p>Exemple :</p> <pre><code>python src/cli/dsba_cli list_s3_datasets\n</code></pre> </li> </ul>"},{"location":"#afficher-les-versions-dun-dataset-s3-specifique","title":"Afficher les versions d\u2019un dataset S3 sp\u00e9cifique","text":"<p>\u2026</p>"},{"location":"#telecharger-un-dataset-de-s3","title":"T\u00e9l\u00e9charger un dataset de S3","text":"<p>Permet de r\u00e9cup\u00e9rer un dataset stock\u00e9 dans le bucket S3 <code>dsba-mlops-project-bucket</code> et de le t\u00e9l\u00e9charger dans l\u2019environnement local.</p> <ul> <li>Param\u00e8tre :<ul> <li><code>--s3_filename</code> (obligatoire) : Nom du fichier tel qu\u2019il est retourn\u00e9 par la commande <code>list_s3_datasets</code>.</li> </ul> </li> <li> <p>Exemple :</p> <pre><code>python src/cli/dsba_cli download_dataset_from_S3 --s3_filename titanic/titanic_v1_2025-03-06_22-29.csv\n</code></pre> </li> </ul>"},{"location":"#pretraiter-un-dataset","title":"Pr\u00e9traiter un dataset","text":"<p>Effectue un pr\u00e9traitement sur un dataset en remplissant les valeurs manquantes selon le mode sp\u00e9cifi\u00e9 et en supprimant les colonnes inutiles.</p> <ul> <li> <p>Param\u00e8tres :</p> <ul> <li><code>--dataset</code> (obligatoire) : Chemin vers le dataset \u00e0 pr\u00e9traiter.</li> <li><code>--target</code> (obligatoire) : Nom de la variable cible.</li> <li><code>--mode</code> (obligatoire) : Mode de remplissage des valeurs manquantes (modes possibles : <code>mean</code>, <code>median</code>, <code>most_frequent</code>, <code>constant</code>)</li> <li><code>--useless</code> (optionnel) : Liste des colonnes \u00e0 exclure du dataset (s\u00e9par\u00e9es par un espace).</li> </ul> <p>Remarque :</p> <ul> <li>Si <code>--useless</code> n'est pas sp\u00e9cifi\u00e9, aucune colonne n'est supprim\u00e9e.</li> <li>Exemple :</li> </ul> <pre><code>python src/cli/dsba_cli preprocess --dataset titanic/titanic_v1_2025-03-01_14-29.csv --target Survived --mode mean --useless PassengerId Name Ticket\n</code></pre> </li> </ul>"},{"location":"#entrainer-un-modele","title":"Entra\u00eener un mod\u00e8le","text":"<p>Permet d\u2019entra\u00eener un mod\u00e8le apr\u00e8s pr\u00e9traitement du dataset.</p> <ul> <li>Param\u00e8tres :<ul> <li><code>--dataset</code> (obligatoire) : Chemin vers le dataset pr\u00e9trait\u00e9.</li> <li><code>--target</code> (obligatoire) : Nom de la variable cible.</li> <li><code>--model</code> (obligatoire) : Mod\u00e8le d\u2019apprentissage \u00e0 utiliser (mod\u00e8les possibles : <code>xgboost</code>, <code>random_forest</code>, <code>logistic_regression</code>, <code>svm</code>, <code>decision_tree</code>, <code>all</code>)</li> <li><code>--gridsearch</code> (optionnel, <code>False</code> par d\u00e9faut) : Indique si un GridSearch doit \u00eatre utilis\u00e9 pour trouver les meilleurs hyperparam\u00e8tres.</li> </ul> </li> <li> <p>Exemple :</p> <ul> <li> <p>Entra\u00eener un mod\u00e8le XGBoost avec GridSearch :</p> <pre><code>python src/cli/dsba_cli train --dataset preprocessed_datasets/titanic/titanic_v1_2025-03-01_14-29 --target Survived --model xgboost --gridsearch\n</code></pre> </li> <li> <p>Entra\u00eener tous les mod\u00e8les disponibles sans GridSearch :</p> <pre><code>python src/cli/dsba_cli train --dataset preprocessed_datasets/titanic/titanic_v1_2025-03-01_14-29 --target Survived --model all\n</code></pre> </li> </ul> </li> </ul>"},{"location":"#pretraiter-un-dataset-et-entrainer-un-modele","title":"Pr\u00e9traiter un dataset et entra\u00eener un mod\u00e8le","text":"<p>Effectue le pr\u00e9traitement d\u2019un dataset et entra\u00eene un mod\u00e8le en une seule commande.</p> <ul> <li>Param\u00e8tres :<ul> <li><code>--dataset</code> (obligatoire) : Chemin vers le dataset \u00e0 pr\u00e9traiter.</li> <li><code>--target</code> (obligatoire) : Nom de la variable cible.</li> <li><code>--mode</code> (obligatoire) : Mode de remplissage des valeurs manquantes (modes possibles : <code>mean</code>, <code>median</code>, <code>most_frequent</code>, <code>constant</code>)</li> <li><code>--useless</code> (optionnel) : Liste des colonnes \u00e0 exclure du dataset (s\u00e9par\u00e9es par un espace).</li> <li><code>--model</code> (obligatoire) : Mod\u00e8le d\u2019apprentissage \u00e0 utiliser (mod\u00e8les possibles : <code>xgboost</code>, <code>random_forest</code>, <code>logistic_regression</code>, <code>svm</code>, <code>decision_tree</code>, <code>all</code>)</li> <li><code>--gridsearch</code> (optionnel, <code>False</code> par d\u00e9faut) : Indique si un GridSearch doit \u00eatre utilis\u00e9 pour trouver les meilleurs hyperparam\u00e8tres.</li> </ul> </li> <li> <p>Exemple :</p> <pre><code>python src/cli/dsba_cli preprocess_and_train --dataset titanic/titanic_v1_2025-03-01_14-29.csv --target Survived --model xgboost --mode mean\n</code></pre> </li> </ul>"},{"location":"#afficher-les-modeles-disponibles","title":"Afficher les mod\u00e8les disponibles","text":"<p>Affiche la liste des mod\u00e8les associ\u00e9s \u00e0 un dataset sp\u00e9cifique.</p> <ul> <li>Param\u00e8tre :<ul> <li><code>--dataset</code> (obligatoire) : Nom du dataset.</li> </ul> </li> <li> <p>Exemple :</p> <pre><code>python src/cli/dsba_cli list_models --dataset titanic\n</code></pre> </li> </ul>"},{"location":"#comparer-les-modeles","title":"Comparer les mod\u00e8les","text":"<p>Affiche les m\u00e9triques des mod\u00e8les.</p> <ul> <li>Param\u00e8tre :<ul> <li><code>--dataset</code> (obligatoire) : Nom du dataset.</li> </ul> </li> <li> <p>Exemple :</p> <pre><code>python src/cli/dsba_cli compare_models --dataset titanic\n</code></pre> </li> </ul>"},{"location":"#trouver-le-meilleur-modele-pour-un-dataset","title":"Trouver le meilleur mod\u00e8le pour un dataset","text":"<p>Teste plusieurs mod\u00e8les et s\u00e9lectionne celui offrant la meilleure performance selon la m\u00e9trique sp\u00e9cifi\u00e9e.</p> <ul> <li>Param\u00e8tres :<ul> <li><code>--dataset</code> (obligatoire) : Nom du dataset.</li> <li><code>--metric</code> (optionnel, <code>f1_score</code> par d\u00e9faut) : M\u00e9trique d\u2019\u00e9valuation du mod\u00e8le (m\u00e9triques disponibles : <code>accuracy</code>, <code>precision</code>, <code>recall</code>, <code>f1_score</code>)</li> </ul> </li> <li> <p>Exemple :</p> <pre><code>python src/cli/dsba_cli find_best_model --dataset titanic --metric f1_score\n</code></pre> </li> <li> <p>Remarque :</p> <p>Le meilleur mod\u00e8le est enregistr\u00e9 dans le fichier <code>best_model.txt</code> compos\u00e9 de 3 lignes :</p> <ul> <li>nom de l\u2019algorithme utilis\u00e9</li> <li>GridSearch utilis\u00e9 ou pas</li> <li>dataset sur lequel a \u00e9t\u00e9 entra\u00een\u00e9 le mod\u00e8le</li> </ul> </li> </ul>"},{"location":"#faire-une-prediction-avec-un-modele-specifique","title":"Faire une pr\u00e9diction avec un mod\u00e8le sp\u00e9cifique","text":"<p>Pr\u00e9dit les r\u00e9sultats \u00e0 partir d\u2019un fichier de test et sauvegarde les pr\u00e9dictions dans un fichier de sortie.</p> <ul> <li>Param\u00e8tres :<ul> <li><code>--input</code> (obligatoire) : Chemin vers le fichier de test.</li> <li><code>--output</code> (obligatoire) : Chemin du fichier de sortie des pr\u00e9dictions.</li> <li><code>--model</code> (obligatoire) : Mod\u00e8le \u00e0 utiliser pour la pr\u00e9diction.</li> </ul> </li> <li> <p>Exemple :</p> <pre><code>python src/cli/dsba_cli predict --input titanic_test.csv --output predictions.csv --model titanic/titanic_v1_2025-03-01_1_random_forest\n</code></pre> </li> </ul>"},{"location":"#faire-une-prediction-avec-le-meilleur-modele","title":"Faire une pr\u00e9diction avec le meilleur mod\u00e8le","text":"<p>Utilise automatiquement le meilleur mod\u00e8le disponible pour faire une pr\u00e9diction.</p> <ul> <li>Param\u00e8tres :<ul> <li><code>--input</code> (obligatoire) : Chemin vers le fichier de test.</li> <li><code>--output</code> (obligatoire) : Chemin du fichier de sortie des pr\u00e9dictions.</li> <li><code>--folder</code> (obligatoire) : Nom du dataset contenant le mod\u00e8le \u00e0 utiliser.</li> </ul> </li> <li> <p>Exemple :</p> <pre><code>python src/cli/dsba_cli predict_with_best_model --input titanic_test.csv --output predictions.csv --folder titanic\n</code></pre> </li> </ul>"},{"location":"#build-image","title":"Build image","text":"<pre><code>python src/cli/dsba_cli build_image\n</code></pre>"},{"location":"#run-container","title":"Run container","text":"<pre><code>python src/cli/dsba_cli run_container\n</code></pre>"},{"location":"#fastapi","title":"FastAPI","text":"<ul> <li>Pr\u00e9-requis : <code>pip install \"fastapi[standard]\"</code></li> <li> <p>Lancer l\u2019API :</p> <pre><code>fastapi dev src/api/api.py\n</code></pre> </li> </ul>"},{"location":"#afficher-la-liste-des-modeles-disponibles-pour-un-dataset-specifique","title":"Afficher la liste des mod\u00e8les disponibles pour un dataset sp\u00e9cifique :","text":"<p>Endpoint : <code>http://127.0.0.1:8000/models/?dataset=&lt;nom_dataset&gt;</code></p>"},{"location":"#recuperer-les-donnees-et-leur-type-a-envoyer-pour-faire-une-requete-a-un-modele","title":"R\u00e9cup\u00e9rer les donn\u00e9es et leur type \u00e0 envoyer pour faire une requ\u00eate \u00e0 un mod\u00e8le","text":"<p>Endpoint : <code>http://127.0.0.1:8000/get_coltypes/?dataset=&lt;nom_dataset&gt;</code></p>"},{"location":"#faire-une-prediction-en-utilisant-un-modele-specifique-entraine-sur-un-dataset-specifique","title":"Faire une pr\u00e9diction en utilisant un mod\u00e8le sp\u00e9cifique entra\u00een\u00e9 sur un dataset sp\u00e9cifique","text":"<p>Endpoint : <code>http://127.0.0.1:8000/predict/</code></p> <ul> <li> <p>Avec cURL :</p> <pre><code>curl -X POST \"http://127.0.0.1:8000/predict/\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n          \"model_id\": \"titanic/titanic_v2_2025-04-01_14-29_random_forest\",\n          \"query\": {\n              \"Pclass\": 3,\n              \"Sex\": 1,\n              \"Age\": 22.0,\n              \"SibSp\": 1,\n              \"Parch\": 0,\n              \"Fare\": 7.25,\n              \"Cabin\": 3,\n              \"Embarked\": 2\n          }\n     }'\n</code></pre> </li> <li> <p>Avec Python :</p> <pre><code>import requests\n\nurl = \"http://127.0.0.1:8000/predict/\"\ndata = {\n        \"model_id\": \"titanic/titanic_v2_2025-04-01_14-29_random_forest\",\n    \"query\": {\n        \"Pclass\": 3,\n        \"Sex\": 1,\n        \"Age\": 22.0,\n        \"SibSp\": 1,\n        \"Parch\": 0,\n        \"Fare\": 7.25,\n        \"Cabin\": 3,\n        \"Embarked\": 2\n    }\n}\nresponse = requests.post(url, json=data)\nprint(response.json())\n</code></pre> </li> </ul>"},{"location":"#faire-une-prediction-en-utilisant-le-meilleur-modele-entraine-sur-un-dataset-specifique","title":"Faire une pr\u00e9diction en utilisant le meilleur mod\u00e8le entra\u00een\u00e9 sur un dataset sp\u00e9cifique","text":"<p>Endpoint : <code>http://127.0.0.1:8000/predict_with_best_model/</code></p> <ul> <li> <p>Avec cURL :</p> <pre><code>curl -X POST \"http://127.0.0.1:8000/predict_with_best_model/\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n          \"model_id\": \"titanic\",\n          \"query\": {\n              \"Pclass\": 3,\n              \"Sex\": 1,\n              \"Age\": 22.0,\n              \"SibSp\": 1,\n              \"Parch\": 0,\n              \"Fare\": 7.25,\n              \"Cabin\": 3,\n              \"Embarked\": 2\n          }\n     }'\n</code></pre> </li> <li> <p>Avec Python :</p> <pre><code>import requests\n\nurl = \"http://127.0.0.1:8000/predict_with_best_model/\"\ndata = {\n        \"model_id\": \"titanic\",\n    \"query\": {\n        \"Pclass\": 3,\n        \"Sex\": 1,\n        \"Age\": 22.0,\n        \"SibSp\": 1,\n        \"Parch\": 0,\n        \"Fare\": 7.25,\n        \"Cabin\": 3,\n        \"Embarked\": 2\n    }\n}\nresponse = requests.post(url, json=data)\nprint(response.json())\n</code></pre> </li> </ul>"},{"location":"#docker","title":"Docker","text":"<p><code>~/Desktop/DSBA/T2 - Data Sciences Electives/MLOps/dsba-platform</code></p> <ul> <li> <p>Build l\u2019image :</p> <ul> <li> <p>Architecture ARM64 :</p> <pre><code>docker build -t fastapi-app -f src/api/Dockerfile .\n</code></pre> </li> <li> <p>Architecture AMD64 :</p> <pre><code>docker buildx build --platform linux/amd64 -t fastapi-app -f src/api/Dockerfile . --load\n</code></pre> </li> </ul> </li> <li> <p>Run le conteneur :</p> <pre><code>docker run -d -p 8000:8000 --name fastapi-container -v \"/Users/allisterkohn/Desktop/DSBA/T2 - Data Sciences Electives/MLOps/Project/dsba-platform/models:/app/models\" -e DSBA_MODELS_ROOT_PATH=\"/app/models\" fastapi-app\n</code></pre> </li> <li> <p>Tagger l\u2019image pour AWS ECR :</p> <pre><code>docker tag fastapi-app 217831684037.dkr.ecr.eu-west-3.amazonaws.com/mlops-app-runner:latest\n</code></pre> </li> <li> <p>S\u2019authentifier aupr\u00e8s d\u2019AWS :</p> <pre><code>aws ecr get-login-password --region eu-west-3 | docker login --username AWS --password-stdin 217831684037.dkr.ecr.eu-west-3.amazonaws.com\n</code></pre> </li> <li> <p>Cr\u00e9er un d\u00e9p\u00f4t sur AWS ECR :</p> <pre><code>aws ecr create-repository --repository-name mlops-app-runner --region eu-west-3\n</code></pre> </li> <li> <p>Pousser l\u2019image sur ce d\u00e9p\u00f4t :</p> <pre><code>docker push 217831684037.dkr.ecr.eu-west-3.amazonaws.com/mlops-app-runner:latest\n</code></pre> </li> <li> <p>Cr\u00e9er un service AWS App Runner :</p> <pre><code>aws apprunner create-service --service-name mlops-app-runner \\\n    --region eu-west-3 \\\n    --profile s3-user \\\n    --source-configuration '{\n        \"AuthenticationConfiguration\": {\n            \"AccessRoleArn\": \"arn:aws:iam::217831684037:role/service-role/AppRunnerECRAccessRole\"\n        },\n        \"ImageRepository\": {\n            \"ImageIdentifier\": \"217831684037.dkr.ecr.eu-west-3.amazonaws.com/mlops-app-runner:latest\",\n            \"ImageRepositoryType\": \"ECR\",\n            \"ImageConfiguration\": {\n                \"Port\": \"8000\",\n                \"RuntimeEnvironmentVariables\": {\n                    \"DSBA_MODELS_ROOT_PATH\": \"/app/models\"\n                }\n            }\n        },\n        \"AutoDeployments\": true\n    }'\n</code></pre> </li> <li> <p>R\u00e9cup\u00e9rer le <code>ServiceURI</code> du service d\u00e9ploy\u00e9 :</p> <pre><code>aws apprunner list-services --region eu-west-3 --profile s3-user\n</code></pre> </li> </ul>"},{"location":"#aws","title":"AWS","text":"<ul> <li>Installer le CLI AWS : <code>brew install awscli</code></li> <li>Se connecter \u00e0 n\u2019importe quel utilisateur de votre compte AWS : <code>aws configure</code></li> <li> <p>Cr\u00e9er un nouvel utilisateur :</p> <pre><code>aws iam create-user --user-name MyIAMUser\n</code></pre> </li> <li> <p>Attacher des politiques \u00e0 cet utilisateur :</p> <pre><code>aws iam attach-user-policy --user-name MyIAMUser --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess\naws iam attach-user-policy --user-name MyIAMUser --policy-arn arn:aws:iam::aws:policy/AmazonEC2ReadOnlyAccess\naws iam attach-user-policy --user-name MyIAMUser --policy-arn arn:aws:iam::aws:policy/AmazonECS_FullAccess\naws iam attach-user-policy --user-name MyIAMUser --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\naws iam attach-user-policy --user-name MyIAMUser --policy-arn arn:aws:iam::aws:policy/AWSAppRunnerFullAccess\naws iam attach-user-policy --user-name MyIAMUser --policy-arn arn:aws:iam::aws:policy/service-role/AWSAppRunnerServicePolicyForECRAccess\naws iam attach-user-policy --user-name MyIAMUser --policy-arn arn:aws:iam::aws:policy/IAMFullAccess\naws iam attach-user-policy --user-name MyIAMUser --policy-arn arn:aws:iam::aws:policy/IAMReadOnlyAccess\n</code></pre> </li> <li> <p>Cr\u00e9er une cl\u00e9 d\u2019acc\u00e8s :</p> <pre><code>aws iam create-access-key --user-name MyIAMUser\n</code></pre> </li> <li> <p>Se connecter \u00e0 l\u2019utilisateur cr\u00e9\u00e9 :</p> <pre><code>aws configure\n</code></pre> </li> </ul>"}]}