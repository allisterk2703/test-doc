{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"developer_documentation/","title":"Developer Documentation","text":""},{"location":"developer_documentation/#fastapi","title":"FastAPI","text":"<ul> <li>Prerequisite: <code>pip install \"fastapi[standard]\"</code></li> <li> <p>Launch the API:</p> <pre><code>fastapi dev src/api/api.py\n</code></pre> </li> </ul>"},{"location":"developer_documentation/#display-the-list-of-available-models-for-a-specific-dataset","title":"Display the list of available models for a specific dataset","text":"<p>Endpoint: <code>http://127.0.0.1:8000/models/?dataset=&lt;dataset_name&gt;</code></p>"},{"location":"developer_documentation/#retrieve-the-data-and-their-types-to-send-for-a-request-to-a-model","title":"Retrieve the data and their types to send for a request to a model","text":"<p>Endpoint: <code>http://127.0.0.1:8000/get_coltypes/?dataset=&lt;dataset_name&gt;</code></p>"},{"location":"developer_documentation/#make-a-prediction-using-a-specific-model-trained-on-a-specific-dataset","title":"Make a prediction using a specific model trained on a specific dataset","text":"<p>Endpoint: <code>http://127.0.0.1:8000/predict/</code></p> <ul> <li> <p>With cURL:</p> <pre><code>curl -X POST \"http://127.0.0.1:8000/predict/\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n          \"model_id\": \"titanic/titanic_v2_2025-04-01_14-29_random_forest\",\n          \"query\": {\n              \"Pclass\": 3,\n              \"Sex\": 1,\n              \"Age\": 22.0,\n              \"SibSp\": 1,\n              \"Parch\": 0,\n              \"Fare\": 7.25,\n              \"Cabin\": 3,\n              \"Embarked\": 2\n          }\n     }'\n</code></pre> </li> <li> <p>With Python:</p> <pre><code>import requests\n\nurl = \"http://127.0.0.1:8000/predict/\"\ndata = {\n    \"model_id\": \"titanic/titanic_v2_2025-04-01_14-29_random_forest\",\n    \"query\": {\n        \"Pclass\": 3,\n        \"Sex\": 1,\n        \"Age\": 22.0,\n        \"SibSp\": 1,\n        \"Parch\": 0,\n        \"Fare\": 7.25,\n        \"Cabin\": 3,\n        \"Embarked\": 2\n    }\n}\nresponse = requests.post(url, json=data)\nprint(response.json())\n</code></pre> </li> </ul>"},{"location":"developer_documentation/#make-a-prediction-using-the-best-model-trained-on-a-specific-dataset","title":"Make a prediction using the best model trained on a specific dataset","text":"<p>Endpoint: <code>http://127.0.0.1:8000/predict_with_best_model/</code></p> <ul> <li> <p>With cURL:</p> <pre><code>curl -X POST \"http://127.0.0.1:8000/predict_with_best_model/\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n          \"model_id\": \"titanic\",\n          \"query\": {\n              \"Pclass\": 3,\n              \"Sex\": 1,\n              \"Age\": 22.0,\n              \"SibSp\": 1,\n              \"Parch\": 0,\n              \"Fare\": 7.25,\n              \"Cabin\": 3,\n              \"Embarked\": 2\n          }\n     }'\n</code></pre> </li> <li> <p>With Python:</p> <pre><code>import requests\n\nurl = \"http://127.0.0.1:8000/predict_with_best_model/\"\ndata = {\n    \"model_id\": \"titanic\",\n    \"query\": {\n        \"Pclass\": 3,\n        \"Sex\": 1,\n        \"Age\": 22.0,\n        \"SibSp\": 1,\n        \"Parch\": 0,\n        \"Fare\": 7.25,\n        \"Cabin\": 3,\n        \"Embarked\": 2\n    }\n}\nresponse = requests.post(url, json=data)\nprint(response.json())\n</code></pre> </li> </ul>"},{"location":"developer_documentation/#docker","title":"Docker","text":"<p><code>~/Desktop/DSBA/T2 - Data Sciences Electives/MLOps/dsba-platform</code></p> <ul> <li> <p>Build the image:</p> <ul> <li> <p>ARM64 architecture:</p> <pre><code>docker build -t fastapi-app -f src/api/Dockerfile .\n</code></pre> </li> <li> <p>AMD64 architecture:</p> <pre><code>docker buildx build --platform linux/amd64 -t fastapi-app -f src/api/Dockerfile . --load\n</code></pre> </li> </ul> </li> <li> <p>Run the container:</p> <pre><code>docker run -d --platform linux/amd64 -p 8000:8000 --name fastapi-container \\\n  -v \"/Users/allisterkohn/Desktop/DSBA/T2 - Data Sciences Electives/MLOps/Project/dsba-platform/models:/app/models\" \\\n  -e DSBA_MODELS_ROOT_PATH=\"/app/models\" \\\n  --env-file .env \\\n  fastapi-app\n</code></pre> <p>(If the .env file is stored somewhere other than the project root, provide the full path)</p> <ul> <li> <p>If the container <code>fastapi-container</code> already exists, use:</p> <pre><code>docker rm -f fastapi-container\n</code></pre> <p>Then re-run the  <code>docker run</code> command.</p> </li> </ul> </li> <li> <p>Display the environment variables of the container:</p> <pre><code>docker exec -it fastapi-container env\n</code></pre> </li> <li> <p>Tag the image for AWS ECR:</p> <pre><code>docker tag fastapi-app 217831684037.dkr.ecr.eu-west-3.amazonaws.com/mlops-app-runner:latest\n</code></pre> </li> <li> <p>Authenticate with AWS:</p> <pre><code>aws ecr get-login-password --region eu-west-3 | docker login --username AWS --password-stdin 217831684037.dkr.ecr.eu-west-3.amazonaws.com\n</code></pre> </li> <li> <p>Create a repository on AWS ECR:</p> <pre><code>aws ecr create-repository --repository-name mlops-app-runner --region eu-west-3\n</code></pre> </li> <li> <p>Push the image to this repository:</p> <pre><code>docker push 217831684037.dkr.ecr.eu-west-3.amazonaws.com/mlops-app-runner:latest\n</code></pre> </li> <li> <p>Create an AWS App Runner service:</p> <pre><code>aws apprunner create-service --service-name mlops-app-runner \\\n    --region eu-west-3 \\\n    --profile s3-user \\\n    --source-configuration '{\n        \"AuthenticationConfiguration\": {\n            \"AccessRoleArn\": \"arn:aws:iam::217831684037:role/service-role/AppRunnerECRAccessRole\"\n        },\n        \"ImageRepository\": {\n            \"ImageIdentifier\": \"217831684037.dkr.ecr.eu-west-3.amazonaws.com/mlops-app-runner:latest\",\n            \"ImageRepositoryType\": \"ECR\",\n            \"ImageConfiguration\": {\n                \"Port\": \"8000\",\n                \"RuntimeEnvironmentVariables\": {\n                    \"DSBA_MODELS_ROOT_PATH\": \"/app/models\"\n                }\n            }\n        },\n        \"AutoDeployments\": true\n    }'\n</code></pre> </li> <li> <p>Retrieve the `ServiceURI of the deployed service:</p> <pre><code>aws apprunner list-services --region eu-west-3 --profile s3-user\n</code></pre> </li> </ul>"},{"location":"developer_documentation/#aws","title":"AWS","text":"<ul> <li>Install the AWS CLI:: <code>brew install awscli</code></li> <li>Connect to any user of your AWS account: <code>aws configure</code></li> <li> <p>Create a new user:</p> <pre><code>aws iam create-user --user-name MyIAMUser\n</code></pre> </li> <li> <p>Attach policies to this user:</p> <pre><code>aws iam attach-user-policy --user-name MyIAMUser --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess\naws iam attach-user-policy --user-name MyIAMUser --policy-arn arn:aws:iam::aws:policy/AmazonEC2ReadOnlyAccess\naws iam attach-user-policy --user-name MyIAMUser --policy-arn arn:aws:iam::aws:policy/AmazonECS_FullAccess\naws iam attach-user-policy --user-name MyIAMUser --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\naws iam attach-user-policy --user-name MyIAMUser --policy-arn arn:aws:iam::aws:policy/AWSAppRunnerFullAccess\naws iam attach-user-policy --user-name MyIAMUser --policy-arn arn:aws:iam::aws:policy/service-role/AWSAppRunnerServicePolicyForECRAccess\naws iam attach-user-policy --user-name MyIAMUser --policy-arn arn:aws:iam::aws:policy/IAMFullAccess\naws iam attach-user-policy --user-name MyIAMUser --policy-arn arn:aws:iam::aws:policy/IAMReadOnlyAccess\n</code></pre> </li> <li> <p>Create an access key:</p> <pre><code>aws iam create-access-key --user-name MyIAMUser\n</code></pre> </li> <li> <p>Connect to the created user:</p> <pre><code>aws configure\n</code></pre> </li> </ul>"},{"location":"installation_setup/","title":"Installation &amp; Setup","text":"<ol> <li>Download the repository<ul> <li>Go to mlops-project.</li> <li>Click on \"Code\", then \"Download ZIP\".</li> <li>Extract the ZIP file to your desired location.</li> </ul> </li> <li>Open the project in VS Code<ul> <li>Open the extracted folder in **VS Code.</li> <li>Open a terminal: Press <code>Control ^</code> + <code>Shift \u21e7</code> + <code>&lt;</code>.</li> </ul> </li> <li> <p>Set up the virtual environment</p> <p>Run the following commands in the terminal:</p> <pre><code>python -m venv .venv  # Create a virtual environment\nsource .venv/bin/activate  # Activate it\npip install hatch  # Install Hatch for environment management\nhatch env create  # Create the environment with Hatch\npip install -e .  # Install project dependencies\n</code></pre> </li> <li> <p>Create a <code>.env</code> file</p> <p>At the root of the project, create a <code>.env</code> file and add the following:</p> <pre><code>AWS_ACCESS_KEY=&lt;YOUR_AWS_ACCESS_KEY&gt;\nAWS_SECRET_KEY=&lt;YOUR_AWS_SECRET_KEY&gt;\nAWS_REGION=&lt;YOUR_AWS_REGION&gt;\nS3_BUCKET_NAME=&lt;YOUR_S3_BUCKET_NAME&gt;\n</code></pre> </li> </ol>"},{"location":"introduction/","title":"Introduction","text":"<p>https://github.com/allisterk2703/mlops-project</p>"},{"location":"introduction/#team-members","title":"Team Members","text":"<p>Allister KOHN, allister.kohn@student-cs.fr</p> <p>Elizaveta VASILEVA, elizaveta.vasileva@student-cs.fr</p> <p>Enzo PALOS, enzo.palos@student-cs.fr</p> <p>Jinsuh YOU, jinsuh.you@student-cs.fr</p>"},{"location":"introduction/#project-description","title":"Project description","text":"<p>The project proposed to us as part of our MLOps course aims to design an MLOps workflow to industrialize the lifecycle of Machine Learning models. A classic lifecycle includes data ingestion, model training, evaluation, and deployment. The goal is to identify and implement key features to facilitate the management, scalability, and reproducibility of models in production. The project is divided into two parts:</p> <ol> <li>Write a specification detailing the chosen workflows and features</li> <li>Implement the corresponding code, to be hosted on GitHub.</li> </ol> <p>The objective is to structure a modular, clear, and reusable system, adopting a pragmatic and robust approach.</p> <p>To this end, the GitHub repository dsba-platform is a relevant starting point, as it offers a modular architecture that integrates the essential steps of an MLOps workflow. It also includes a CLI, an API (with FastAPI), and a Dockerfile, facilitating containerization, integration, and deployment to the cloud. Furthermore, its structure easily allows for adding advanced features that ensure a smooth and reproducible workflow.</p> <p>The code in this repository is suited to binary classification tasks, like the famous Titanic dataset, which predicts passenger survival based on their characteristics. Consequently, we have chosen to focus on a tool intended exclusively for binary classification tasks in order to remain focused on a clear and well-controlled scope.</p>"},{"location":"user_guide/","title":"User Guide","text":""},{"location":"user_guide/#features","title":"Features","text":""},{"location":"user_guide/#add-a-dataset","title":"Add a Dataset","text":"<p>Allows adding a dataset to the tool.</p> <ul> <li>Parameters:<ul> <li><code>--name</code> (required): Name assigned to the dataset when adding it.</li> <li><code>--file</code>: Path to a local file containing the dataset.</li> <li><code>--url</code>: URL of a remote source to fetch the dataset.</li> <li><code>--new</code>: Indicates that this is a new dataset (should not be included when adding a new version of an existing dataset).</li> </ul> </li> <li>Notes:<ul> <li>Either <code>--file</code> or <code>--url</code> must be specified, but not both simultaneously.</li> <li>The <code>--new</code> flag is required to add a dataset that does not yet exist.</li> </ul> </li> <li> <p>Exemples :</p> <ul> <li> <p>Add a dataset from a local file:</p> <pre><code>python src/cli/dsba_cli save_dataset --name titanic --file /Users/allisterkohn/Desktop/titanic.csv\n</code></pre> </li> <li> <p>Add a dataset from a URL:</p> <pre><code>python src/cli/dsba_cli save_dataset --name titanic --url https://www.kaggle.com/api/v1/datasets/download/yasserh/titanic-dataset\n</code></pre> </li> </ul> </li> </ul>"},{"location":"user_guide/#display-all-local-datasets","title":"Display All Local Datasets","text":"<p>Displays the list of all datasets stored locally on the machine.</p> <ul> <li> <p>Example:</p> <pre><code>python src/cli/dsba_cli list_local_datasets\n</code></pre> </li> </ul>"},{"location":"user_guide/#display-versions-of-a-specific-local-dataset","title":"Display Versions of a Specific Local Dataset","text":"<p>Lists all versions of a specific dataset stored locally.</p> <ul> <li>Parameter:<ul> <li><code>--dataset</code> (required): Name of the dataset.</li> </ul> </li> <li> <p>Example:</p> <pre><code>python src/cli/dsba_cli list_dataset_versions --dataset titanic\n</code></pre> </li> </ul>"},{"location":"user_guide/#display-all-s3-datasets","title":"Display All S3 Datasets","text":"<p>Displays the list of datasets stored in the S3 bucket <code>dsba-mlops-project-bucket</code>, available for download.</p> <ul> <li> <p>Example:</p> <pre><code>python src/cli/dsba_cli list_s3_datasets\n</code></pre> </li> </ul>"},{"location":"user_guide/#display-versions-of-a-specific-s3-dataset","title":"Display Versions of a Specific S3 Dataset \u2026","text":"<p>Lists all versions of a specific dataset stored in the S3 bucket.</p> <ul> <li>Parameter:<ul> <li><code>--dataset</code> (required): Name of the dataset.</li> </ul> </li> <li> <p>Example:</p> <pre><code>python src/cli/dsba_cli list_s3_dataset_versions --dataset titanic\n</code></pre> </li> </ul>"},{"location":"user_guide/#download-a-dataset-from-s3","title":"Download a Dataset from S3","text":"<p>Allows retrieving a dataset stored in the S3 bucket <code>dsba-mlops-project-bucket</code> and downloading it to the local environment.</p> <ul> <li>Parameter:<ul> <li><code>--s3_filename</code> (required): Name of the file as returned by the <code>list_s3_datasets</code> command.</li> </ul> </li> <li> <p>Example:</p> <pre><code>python src/cli/dsba_cli download_dataset_from_S3 --s3_filename titanic/titanic_v1_2025-03-06_22-29.csv\n</code></pre> </li> </ul>"},{"location":"user_guide/#preprocess-a-dataset","title":"Preprocess a Dataset","text":"<p>Performs preprocessing on a dataset by filling missing values according to the specified mode and removing unnecessary columns.</p> <ul> <li> <p>Parameters:</p> <ul> <li><code>--dataset</code> (required): Path to the dataset to preprocess.</li> <li><code>--target</code> (required): Name of the target variable.</li> <li><code>--mode</code> (required): Method for filling missing values (available modes: <code>mean</code>, <code>median</code>, <code>most_frequent</code>, <code>constant</code>).</li> <li><code>--useless</code> (optional): List of columns to exclude from the dataset (separated by spaces).</li> </ul> <p>Note:</p> <ul> <li>If <code>--useless</code> is not specified, no columns will be removed.</li> <li>Example:</li> </ul> <pre><code>python src/cli/dsba_cli preprocess --dataset titanic/titanic_v1_2025-03-01_14-29.csv --target Survived --mode mean --useless PassengerId Name Ticket\n</code></pre> </li> </ul>"},{"location":"user_guide/#train-a-model","title":"Train a Model","text":"<p>Allows training a model after dataset preprocessing.</p> <ul> <li>Parameters:<ul> <li><code>--dataset</code> (required): Path to the preprocessed dataset.</li> <li><code>--target</code> (required): Name of the target variable.</li> <li><code>--model</code> (required): Machine learning model to use (available models: <code>xgboost</code>, <code>random_forest</code>, <code>logistic_regression</code>, <code>svm</code>, <code>decision_tree</code>, <code>all</code>).</li> <li><code>--gridsearch</code> (optional, default: <code>False</code>): Indicates whether to use GridSearch to find the best hyperparameters.</li> </ul> </li> <li> <p>Examples:</p> <ul> <li> <p>Train an XGBoost model with GridSearch:</p> <pre><code>python src/cli/dsba_cli train --dataset preprocessed_datasets/titanic/titanic_v1_2025-03-01_14-29 --target Survived --model xgboost --gridsearch\n</code></pre> </li> <li> <p>Train all available models without GridSearch:</p> <pre><code>python src/cli/dsba_cli train --dataset preprocessed_datasets/titanic/titanic_v1_2025-03-01_14-29 --target Survived --model all\n</code></pre> </li> </ul> </li> </ul>"},{"location":"user_guide/#preprocess-a-dataset-and-train-a-model","title":"Preprocess a Dataset and Train a Model","text":"<p>Performs dataset preprocessing and trains a model in a single command.</p> <ul> <li>Parameters:<ul> <li><code>--dataset</code> (required): Path to the dataset to preprocess.</li> <li><code>--target</code> (required): Name of the target variable.</li> <li><code>--mode</code> (required): Method for filling missing values (available modes: <code>mean</code>, <code>median</code>, <code>most_frequent</code>, <code>constant</code>).</li> <li><code>--useless</code> (optional): List of columns to exclude from the dataset (separated by spaces).</li> <li><code>--model</code> (required): Machine learning model to use (available models: <code>xgboost</code>, <code>random_forest</code>, <code>logistic_regression</code>, <code>svm</code>, <code>decision_tree</code>, <code>all</code>).</li> <li><code>--gridsearch</code> (optional, default: <code>False</code>): Indicates whether to use GridSearch to find the best hyperparameters.</li> </ul> </li> <li> <p>Example:</p> <pre><code>python src/cli/dsba_cli preprocess_and_train --dataset titanic/titanic_v1_2025-03-01_14-29.csv --target Survived --model xgboost --mode mean\n</code></pre> </li> </ul>"},{"location":"user_guide/#list-available-models","title":"List Available Models","text":"<p>Displays the list of models associated with a specific dataset.</p> <ul> <li>Parameter:<ul> <li><code>--dataset</code> (required): Name of the dataset.</li> </ul> </li> <li> <p>Example:</p> <pre><code>python src/cli/dsba_cli list_models --dataset titanic\n</code></pre> </li> </ul>"},{"location":"user_guide/#compare-models","title":"Compare Models","text":"<p>Displays the performance metrics of trained models.</p> <ul> <li>Parameter:<ul> <li><code>--dataset</code> (required): Name of the dataset.</li> </ul> </li> <li> <p>Example:</p> <pre><code>python src/cli/dsba_cli compare_models --dataset titanic\n</code></pre> </li> </ul>"},{"location":"user_guide/#find-the-best-model-for-a-dataset","title":"Find the Best Model for a Dataset","text":"<p>Tests multiple models and selects the one with the best performance based on the specified metric.</p> <ul> <li>Parameters:<ul> <li><code>--dataset</code> (required): Name of the dataset.</li> <li><code>--metric</code> (optional, default: <code>f1_score</code>): Model evaluation metric (available metrics: <code>accuracy</code>, <code>precision</code>, <code>recall</code>, <code>f1_score</code>).</li> </ul> </li> <li> <p>Example:</p> <pre><code>python src/cli/dsba_cli find_best_model --dataset titanic --metric f1_score\n</code></pre> </li> <li> <p>Notes:</p> <p>The best model is saved in the <code>best_model.txt</code> file, which consists of three lines:</p> <ul> <li>The name of the algorithm used.</li> <li>Whether GridSearch was used or not.</li> <li>The dataset on which the model was trained.</li> </ul> </li> </ul>"},{"location":"user_guide/#make-a-prediction-with-a-specific-model","title":"Make a Prediction with a Specific Model","text":"<p>Predicts results from a test file and saves the predictions to an output file.</p> <ul> <li>Parameters:<ul> <li><code>--input</code> (required): Path to the test file.</li> <li><code>--output</code> (required): Path to the output file for the predictions.</li> <li><code>--model</code> (required): Model to use for the prediction.</li> </ul> </li> <li> <p>Example:</p> <pre><code>python src/cli/dsba_cli predict --input titanic_test.csv --output predictions.csv --model titanic/titanic_v1_2025-03-01_1_random_forest\n</code></pre> </li> </ul>"},{"location":"user_guide/#make-a-prediction-with-the-best-model","title":"Make a Prediction with the Best Model","text":"<p>Automatically uses the best available model to make a prediction.</p> <ul> <li>Parameters:<ul> <li><code>--input</code> (required): Path to the test file.</li> <li><code>--output</code> (required): Path to the output file for the predictions.</li> <li><code>--folder</code> (required): Name of the dataset containing the model to use.</li> </ul> </li> <li> <p>Example:</p> <pre><code>python src/cli/dsba_cli predict_with_best_model --input titanic_test.csv --output predictions.csv --folder titanic\n</code></pre> </li> </ul>"},{"location":"user_guide/#build-image","title":"Build image","text":"<p>Builds a Docker image for the project.</p> <pre><code>python src/cli/dsba_cli build_image\n</code></pre>"},{"location":"user_guide/#run-container","title":"Run container","text":"<p>Runs the Docker container.</p> <pre><code>python src/cli/dsba_cli run_container\n</code></pre>"}]}